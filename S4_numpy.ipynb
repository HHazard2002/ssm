{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Model:\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        # Initialize learnable parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.A = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "        self.B = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.C = np.random.randn(num_classes, hidden_size) * 0.1\n",
    "        self.D = np.random.randn(num_classes, input_size) * 0.1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the S4 model.\n",
    "        x: Input sequence of shape (seq_len, input_size)\n",
    "        Returns: Output logits of shape (num_classes,)\n",
    "        \"\"\"\n",
    "        seq_len, input_size = x.shape\n",
    "        h = np.zeros((self.hidden_size,))  # Initialize hidden state\n",
    "        for t in range(seq_len):\n",
    "            u_t = x[t]  # Input at time t\n",
    "            h = np.tanh(np.dot(self.A, h) + np.dot(self.B, u_t))  # Update hidden state\n",
    "        y = np.dot(self.C, h) + np.dot(self.D, u_t)  # Compute output logits\n",
    "        return y\n",
    "\n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return np.argmax(logits)  # Return the predicted class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, label):\n",
    "    exp_logits = np.exp(logits - np.max(logits))  # Numerical stability\n",
    "    probs = exp_logits / np.sum(exp_logits)\n",
    "    return -np.log(probs[label]), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(model, grads, lr):\n",
    "    for param, grad in grads.items():\n",
    "        model.__dict__[param] -= lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model, x, label):\n",
    "    \"\"\"\n",
    "    Backpropagation for S4.\n",
    "    Returns gradients for A, B, C, D.\n",
    "    \"\"\"\n",
    "    seq_len, input_size = x.shape\n",
    "    h = np.zeros((model.hidden_size,))\n",
    "    hs = []  # Store hidden states for backpropagation\n",
    "\n",
    "    # Forward pass\n",
    "    for t in range(seq_len):\n",
    "        u_t = x[t]\n",
    "        h = np.tanh(np.dot(model.A, h) + np.dot(model.B, u_t))\n",
    "        hs.append(h)\n",
    "    logits = np.dot(model.C, h) + np.dot(model.D, u_t)\n",
    "\n",
    "    # Compute loss and probabilities\n",
    "    loss, probs = cross_entropy_loss(logits, label)\n",
    "\n",
    "    # Gradients initialization\n",
    "    d_logits = probs\n",
    "    d_logits[label] -= 1  # Gradient of cross-entropy wrt logits\n",
    "    d_C = np.outer(d_logits, hs[-1])\n",
    "    d_D = np.outer(d_logits, x[-1])\n",
    "\n",
    "    d_h = np.dot(model.C.T, d_logits)\n",
    "\n",
    "    d_A = np.zeros_like(model.A)\n",
    "    d_B = np.zeros_like(model.B)\n",
    "    for t in reversed(range(seq_len)):\n",
    "        u_t = x[t]\n",
    "        h = hs[t]\n",
    "        d_tanh = (1 - h ** 2) * d_h  # Gradient through tanh\n",
    "        d_A += np.outer(d_tanh, hs[t - 1] if t > 0 else np.zeros_like(h))\n",
    "        d_B += np.outer(d_tanh, u_t)\n",
    "        d_h = np.dot(model.A.T, d_tanh)\n",
    "\n",
    "    grads = {\n",
    "        'A': d_A,\n",
    "        'B': d_B,\n",
    "        'C': d_C,\n",
    "        'D': d_D,\n",
    "    }\n",
    "    return loss, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Function to convert DataLoader batches to NumPy\n",
    "def dataloader_to_numpy(data_loader):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for batch_images, batch_labels in data_loader:\n",
    "        # Move to CPU and convert to NumPy\n",
    "        data.append(batch_images.numpy())\n",
    "        labels.append(batch_labels.numpy())\n",
    "    # Concatenate batches\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return data, labels\n",
    "\n",
    "# Convert train and test data\n",
    "train_images, train_labels = dataloader_to_numpy(train_loader)\n",
    "test_images, test_labels = dataloader_to_numpy(test_loader)\n",
    "\n",
    "# Reshape the images into sequences of length 28 (seq_len=28, input_size=28)\n",
    "train_images = train_images.reshape(-1, 28, 28)\n",
    "test_images = test_images.reshape(-1, 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.5155643616332608\n",
      "Epoch 2/5, Loss: 0.21744363777599113\n",
      "Epoch 3/5, Loss: 0.172894513521342\n",
      "Epoch 4/5, Loss: 0.1398888802354398\n",
      "Epoch 5/5, Loss: 0.12673826988745282\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and hyperparameters\n",
    "model = S4Model(input_size=28, hidden_size=128, num_classes=10)\n",
    "learning_rate = 0.0005\n",
    "epochs = 5\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(train_images)):\n",
    "        x = train_images[i]  # Shape: (28, 28)\n",
    "        label = train_labels[i]\n",
    "\n",
    "        # Compute loss and gradients\n",
    "        loss, grads = compute_gradients(model, x, label)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Update parameters\n",
    "        update_params(model, grads, learning_rate)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_images)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.23%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test Accuracy\n",
    "correct = 0\n",
    "for i in range(len(test_images)):\n",
    "    x = test_images[i]\n",
    "    label = test_labels[i]\n",
    "    pred = model.predict(x)\n",
    "    correct += (pred == label)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / len(test_images) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
